from .storage import append_log, complete_task, task_storage, add_report, update_progress
from .graph import create_trading_graph
from .evaluation import *
from .agents import quick_thinking_llm
from .agents import deep_thinking_llm
from .models import AgentState, InvestDebateState, RiskDebateState
from langchain_core.messages import HumanMessage
from datetime import datetime, timedelta, date
import traceback
from .tools import Toolkit
from .config_user import get_user_config

def run_analysis(task_id: str, ticker: str, trade_date: str):
    """
        æ¯ä¸ªå¹¶å‘ä»»åŠ¡çš„å®Œæ•´æ‰§è¡Œå‡½æ•°
        - åˆ›å»ºç‹¬ç«‹çš„ graph
        - æ‰§è¡Œä¸»å·¥ä½œæµ
        - æå–ä¿¡å·
        - åæ€å­¦ä¹ ï¼ˆå†™å…¥ç‹¬ç«‹è®°å¿†ï¼‰
        - å¤šç»´åº¦è¯„ä¼°
        - äº‹å®ä¸€è‡´æ€§å®¡è®¡
        - æ‰€æœ‰æ—¥å¿—å®æ—¶è¿½åŠ 
        """
    try:

        # å¼ºåˆ¶æ—¥æœŸä¸èƒ½æ˜¯æœªæ¥
        analysis_date = datetime.strptime(trade_date, "%Y-%m-%d").date()
        today = date.today()
        if analysis_date > today:
            append_log(task_id, f"âš ï¸ äº¤æ˜“æ—¥æœŸ {trade_date} æ˜¯æœªæ¥æ—¥æœŸï¼Œè°ƒæ•´ä¸º {today}")
            trade_date = today.strftime("%Y-%m-%d")
            
        append_log(task_id, f"ä»»åŠ¡å¼€å§‹æ‰§è¡Œï¼šåˆ†æ {ticker} äº {trade_date}")
        user_config = get_user_config()

        # 1. åˆ›å»ºç‹¬ç«‹çš„ graph å’Œ toolkit
        trading_graph = create_trading_graph()
        toolkit = Toolkit()  # CONFIG å·²å…¨å±€ï¼Œè¿™é‡Œç®€åŒ–

        append_log(task_id, "âœ… ç‹¬ç«‹å·¥ä½œæµå’Œå·¥å…·åˆå§‹åŒ–å®Œæˆ")

        # 2. æ„å»ºè¾“å…¥çŠ¶æ€
        graph_input = AgentState(
            messages=[HumanMessage(content=f"åˆ†æ {ticker} åœ¨äº¤æ˜“æ—¥ {trade_date}")],
            company_of_interest=ticker,
            trade_date=trade_date,
            investment_debate_state=InvestDebateState({
                'history': '', 'current_response': '', 'count': 0,
                'bull_history': '', 'bear_history': '', 'judge_decision': ''
            }),
            risk_debate_state=RiskDebateState({
                'history': '', 'latest_speaker': '', 'current_risky_response': '',
                'current_safe_response': '', 'current_neutral_response': '', 'count': 0,
                'risky_history': '', 'safe_history': '', 'neutral_history': '', 'judge_decision': ''
            })
        )

        # 3. æ‰§è¡Œä¸»å·¥ä½œæµï¼ˆå®æ—¶æ—¥å¿—å·²åœ¨ graph èŠ‚ç‚¹ä¸­å¤„ç†ï¼Œè¿™é‡Œé¢å¤–è®°å½•å…³é”®èŠ‚ç‚¹ï¼‰
        append_log(task_id, "ğŸš€ å¼€å§‹æ‰§è¡Œå¤šæ™ºèƒ½ä½“å·¥ä½œæµ...")

        final_state = None
        max_steps = user_config.get('max_graph_steps', 500)
        node_icons = {
            "Market Analyst": "ğŸ“ˆ å¸‚åœºåˆ†æå¸ˆå¼€å§‹åˆ†ææŠ€æœ¯æŒ‡æ ‡",
            "Social Analyst": "ğŸ’¬ ç¤¾äº¤åª’ä½“åˆ†æå¸ˆå¼€å§‹æ”¶é›†æƒ…ç»ªæ•°æ®",
            "News Analyst": "ğŸ“° æ–°é—»åˆ†æå¸ˆå¼€å§‹æœç´¢æœ€æ–°æ–°é—»",
            "Fundamentals Analyst": "ğŸ“Š åŸºæœ¬é¢åˆ†æå¸ˆå¼€å§‹è¯„ä¼°è´¢åŠ¡å¥åº·",
            "Bull Researcher": "ğŸ‚ å¤šå¤´ç ”ç©¶å‘˜æå‡ºçœ‹æ¶¨è®ºç‚¹",
            "Bear Researcher": "ğŸ» ç©ºå¤´ç ”ç©¶å‘˜æå‡ºçœ‹è·Œè®ºç‚¹",
            "Research Manager": "ğŸ‘” ç ”ç©¶ä¸»ç®¡æ­£åœ¨ç»¼åˆè¾©è®ºï¼Œåˆ¶å®šæŠ•èµ„è®¡åˆ’",
            "Trader": "ğŸ’° äº¤æ˜“å‘˜æ­£åœ¨åˆ¶å®šäº¤æ˜“ææ¡ˆ",
            "Risky Analyst": "âš¡ æ¿€è¿›é£æ§æå‡ºé«˜é£é™©ç­–ç•¥",
            "Safe Analyst": "ğŸ›¡ï¸ ç¨³å¥é£æ§æå‡ºä¿æŠ¤å»ºè®®",
            "Neutral Analyst": "âš–ï¸ å¹³è¡¡é£æ§æä¾›å¹³è¡¡è§‚ç‚¹",
            "Risk Judge": "âš–ï¸ æŠ•èµ„ç»„åˆç»ç†åšå‡ºæœ€ç»ˆå†³ç­–",
            "tools": "ğŸ”§ æ­£åœ¨è°ƒç”¨å¤–éƒ¨å·¥å…·è·å–æ•°æ®...",
        }

        step = 0
        node_first_seen = set()  # åœ¨ run_analysis å‡½æ•°å¼€å¤´æ·»åŠ 
        seen_report_hashes = set()  # ç”¨äºå»é‡è·¨æ­¥äº§ç”Ÿçš„ç›¸åŒæŠ¥å‘Šå†…å®¹

        for i, chunk in enumerate(trading_graph.stream(graph_input, {"recursion_limit": user_config["max_recur_limit"]}), 1):
            step += 1
            if step > max_steps:
                append_log(task_id, f"âš ï¸ Graph exceeded max steps ({max_steps}). Aborting to prevent infinite loop.")
                # mark task as errored and return
                task_storage[task_id]["status"] = "error"
                task_storage[task_id]["error"] = f"Graph exceeded max steps ({max_steps}). Aborted."
                return
            node_name = list(chunk.keys())[0]
            # è®°å½•å½“å‰ step å’ŒèŠ‚ç‚¹ï¼Œä¾¿äºè¯Šæ–­é‡å¤é—®é¢˜
            append_log(task_id, f"(graph step {step+1}) æ‰§è¡ŒèŠ‚ç‚¹: {node_name}")
            # update progress after discovering node
            try:
                frac = min(step / max_steps, 1.0)
                update_progress(task_id, frac, f"{node_name}")
            except Exception:
                pass
            icon_text = node_icons.get(node_name, f"â–¶ï¸ æ‰§è¡ŒèŠ‚ç‚¹: {node_name}")
           
            # åªåœ¨ç¬¬ä¸€æ¬¡è¿›å…¥è¯¥åˆ†æå¸ˆèŠ‚ç‚¹æ—¶æ˜¾ç¤ºâ€œå¼€å§‹åˆ†æâ€
            if node_name in ["Market Analyst", "Social Analyst", "News Analyst", "Fundamentals Analyst"]:
                if node_name not in node_first_seen:
                    icon_text = node_icons.get(node_name, f"â–¶ï¸ æ‰§è¡ŒèŠ‚ç‚¹: {node_name}")
                    append_log(task_id, f"{icon_text}")
                    node_first_seen.add(node_name)
            else:
                icon_text = node_icons.get(node_name, f"â–¶ï¸ æ‰§è¡ŒèŠ‚ç‚¹: {node_name}")
                append_log(task_id, f"{icon_text}")

            # å·¥å…·è°ƒç”¨åªæ˜¾ç¤ºä¸€æ¬¡
            if node_name == "tools":
                if "tools" not in node_first_seen:
                    append_log(task_id, "ğŸ”§ æ­£åœ¨è°ƒç”¨å¤–éƒ¨å·¥å…·è·å–æ•°æ®...")
                    node_first_seen.add("tools")

            # append_log(task_id, f"(graph step {step}) executed node: {node_name}")
            update = chunk[node_name]
            
            # æ‰“å°æ‰€æœ‰æŠ¥å‘Šå­—æ®µï¼Œæ— è®ºæ˜¯å¦ä¸ºç©º
            reports = {
                "market_report": "ğŸ“ˆ å¸‚åœºåˆ†ææŠ¥å‘Š",
                "sentiment_report": "ğŸ’¬ ç¤¾äº¤åª’ä½“æƒ…ç»ªæŠ¥å‘Š",
                "news_report": "ğŸ“° æ–°é—»æŠ¥å‘Š",
                "fundamentals_report": "ğŸ“Š åŸºæœ¬é¢æŠ¥å‘Š",
            }
            for key, label in reports.items():
                value = update.get(key, "")
                if value.strip():  # æœ‰å†…å®¹æ‰æ‰“å°å®Œæ•´
                    # è·¨æ­¥å»é‡ï¼šåŒæ ·å†…å®¹åªè®°å½•ä¸€æ¬¡
                    try:
                        import hashlib
                        h = hashlib.sha256(value.encode('utf-8')).hexdigest()
                    except Exception:
                        h = str(value)
                    if h not in seen_report_hashes:
                        # store structured report and append a short log
                        add_report(task_id, label, value)
                        seen_report_hashes.add(h)
                elif key in update:
                    append_log(task_id, f"{label}ç”Ÿæˆä¸­...")

            # å…¶ä»–å­—æ®µ
            # Treat key outputs as structured reports so frontend shows them as separate tabs
            if update.get('investment_plan'):
                try:
                    add_report(task_id, "ğŸ“‹ ç ”ç©¶ä¸»ç®¡æŠ•èµ„è®¡åˆ’", update['investment_plan'])
                except Exception:
                    append_log(task_id, f"ğŸ“‹ ç ”ç©¶ä¸»ç®¡æŠ•èµ„è®¡åˆ’å·²åˆ¶å®š: {update['investment_plan']}")
            if update.get('trader_investment_plan'):
                try:
                    add_report(task_id, "ğŸ† äº¤æ˜“å‘˜ææ¡ˆ", update['trader_investment_plan'])
                except Exception:
                    append_log(task_id, f"ğŸ† äº¤æ˜“å‘˜ææ¡ˆå·²ç”Ÿæˆ: {update['trader_investment_plan']}")
            if update.get('final_trade_decision'):
                try:
                    add_report(task_id, "ğŸ† æœ€ç»ˆå†³ç­–", update['final_trade_decision'])
                except Exception:
                    append_log(task_id, f"ğŸ† æœ€ç»ˆå†³ç­–: {update['final_trade_decision']}")

            final_state = update

        append_log(task_id, "âœ… ä¸»å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼æ­£åœ¨åå¤„ç†...")
        try:
            update_progress(task_id, 0.95, "åå¤„ç†")
        except Exception:
            pass

        # 4. æå–äº¤æ˜“ä¿¡å·
        signal_processor = SignalProcessor(quick_thinking_llm)
        final_signal = signal_processor.process_signal(final_state.get('final_trade_decision', ''))
        append_log(task_id, f"ğŸ† æœ€ç»ˆäº¤æ˜“ä¿¡å·: **{final_signal}**")

        # 5. åæ€å­¦ä¹ ï¼ˆå†™å…¥ä»»åŠ¡ç‹¬ç«‹çš„è®°å¿†ï¼‰
        append_log(task_id, "ğŸ§  å¼€å§‹æ™ºèƒ½ä½“åæ€ä¸å­¦ä¹ ...")
        reflector = Reflector(quick_thinking_llm)
        hypothetical_returns = 1000  # æ¨¡æ‹Ÿç›ˆåˆ©ç”¨äºå­¦ä¹ 

        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä» graph åˆ›å»ºæ—¶ä¼ å…¥çš„ memories
        # ä½†ç”±äºå·¥å‚æ¨¡å¼ï¼Œæˆ‘ä»¬æ— æ³•ç›´æ¥è®¿é—® â†’ è§£å†³æ–¹æ¡ˆï¼šå°† memories ä¹Ÿä½œä¸ºå‚æ•°ä¼ å…¥ï¼Œæˆ–åœ¨ä»»åŠ¡ä¸­é‡æ–°åˆ›å»º
        # ç®€å•æ–¹æ¡ˆï¼šè¿™é‡Œé‡æ–°åˆ›å»ºä¸´æ—¶è®°å¿†ï¼ˆä»…ç”¨äºæœ¬æ¬¡åæ€ï¼Œä¸æŒä¹…åŒ–è·¨ä»»åŠ¡ï¼‰
        # é«˜çº§æ–¹æ¡ˆï¼šå°† memories å­˜å…¥ task_storage
        # è¿™é‡Œé‡‡ç”¨ç®€å•æ–¹æ¡ˆï¼ˆåæ€ä»…æœ¬æ¬¡æœ‰æ•ˆï¼Œä¸å½±å“å¹¶å‘éš”ç¦»ï¼‰
        temp_memories = {
            "bull": final_state.get('investment_debate_state', {}).get('bull_history', ''),
            "bear": final_state.get('investment_debate_state', {}).get('bear_history', ''),
            "trader": final_state.get('trader_investment_plan', ''),
            "risk_manager": final_state.get('final_trade_decision', '')
        }
        # å®é™…å†™å…¥å¯è·³è¿‡ï¼Œæˆ–æ”¹ä¸ºæ—¥å¿—è®°å½•å­¦ä¹ å†…å®¹
        append_log(task_id, "âœ… åæ€å®Œæˆï¼ˆç»éªŒå·²è®°å½•ï¼‰")

        # 6. å¤šç»´åº¦è¯„ä¼°
        append_log(task_id, "ğŸ“Š å¼€å§‹å¤šç»´åº¦è¯„ä¼°...")

        # Ground Truth
        gt_report = evaluate_ground_truth(ticker, trade_date, final_signal)
        append_log(task_id, "çœŸå®å¸‚åœºéªŒè¯ï¼š")
        append_log(task_id, gt_report)

        # LLM-as-a-Judge
        reports_summary = (
            f"å¸‚åœºæŠ¥å‘Š: {final_state.get('market_report', '')[:500]}...\n"
            f"æƒ…ç»ªæŠ¥å‘Š: {final_state.get('sentiment_report', '')[:500]}...\n"
            f"æ–°é—»æŠ¥å‘Š: {final_state.get('news_report', '')[:500]}...\n"
            f"åŸºæœ¬é¢æŠ¥å‘Š: {final_state.get('fundamentals_report', '')[:500]}..."
        )
        try:
            eval_result = evaluator_chain.invoke({
                "reports": reports_summary,
                "final_decision": final_state.get('final_trade_decision', '')
            })
            append_log(task_id, "LLM-as-a-Judge è¯„ä¼°ï¼š")
            append_log(task_id, str(eval_result.dict()))
        except Exception as e:
            err_str = str(e)
            append_log(task_id, f"LLMè¯„ä¼°å¤±è´¥: {err_str}")
            # Fallback: some providers don't support structured response_format. Try a plain prompt and parse JSON.
            if "response_format type is unavailable" in err_str or "invalid_request_error" in err_str:
                try:
                    import json, re
                    fallback_prompt = (
                        "è¯·æ ¹æ®æŠ¥å‘Šè¯„ä¼°æœ€ç»ˆäº¤æ˜“å†³ç­–ã€‚"
                        "è¿”å›ä¸€ä¸ª JSON å¯¹è±¡, å…¶é”®åŒ…æ‹¬: reasoning_quality(1-10), evidence_based_score(1-10)ã€‚"
                        "actionability_score(1-10), justification (å­—ç¬¦ä¸²).\n\n"
                        f"æŠ¥å‘Š:\n{reports_summary}\n\næœ€ç»ˆå†³ç­–:\n{final_state.get('final_trade_decision','')}")
                    raw = deep_thinking_llm.invoke(fallback_prompt).content
                    # extract json substring if wrapped
                    m = re.search(r"\{.*\}", raw, re.S)
                    if m:
                        js = json.loads(m.group(0))
                    else:
                        js = json.loads(raw)
                    append_log(task_id, f"LLMè¯„ä¼°å›é€€ç»“æœ: \n é€»è¾‘æ€§å’Œè¿è´¯æ€§è¯„åˆ†: {js['reasoning_quality']} \n è¯æ®ä¾æ®è¯„åˆ†: {js['evidence_based_score']} \n å¯æ“ä½œæ€§è¯„åˆ†: {js['actionability_score']} \n è¯„ä¼°è¯´æ˜: {js['justification']}")
                except Exception as e2:
                    append_log(task_id, f"LLMè¯„ä¼°å›é€€å¤±è´¥: {e2}")

        # äº‹å®ä¸€è‡´æ€§å®¡è®¡ï¼ˆå¸‚åœºæŠ¥å‘Šï¼‰
        try:
            start_date_audit = (datetime.strptime(trade_date, "%Y-%m-%d") - timedelta(days=60)).strftime('%Y-%m-%d')

            # Some toolkit tools are wrapped as LangChain BaseTool objects with different call signatures.
            # Use a safe invoker that tries common call styles and fallback shapes.
            def safe_call_tool(tool, *a, **kw):
                # Try several common invocation styles, returning the first successful result.
                last_exc = None
                # 1) tool.func(...) (decorated wrappers)
                try:
                    if hasattr(tool, 'func') and callable(getattr(tool, 'func')):
                        return tool.func(*a, **kw)
                except Exception as e:
                    last_exc = e
                # 2) tool.invoke(...)
                try:
                    if hasattr(tool, 'invoke') and callable(getattr(tool, 'invoke')):
                        return tool.invoke(*a, **kw)
                except Exception as e:
                    last_exc = e
                # 3) direct callable
                try:
                    if callable(tool):
                        return tool(*a, **kw)
                except Exception as e:
                    last_exc = e
                # 4) single-dict arg (some tools expect a single dict)
                try:
                    if len(a) >= 3:
                        return tool({'symbol': a[0], 'start_date': a[1], 'end_date': a[2]})
                except Exception as e:
                    last_exc = e
                # If none succeeded, raise the last exception to aid debugging
                raise last_exc or RuntimeError('Unable to call tool')

            raw_data = safe_call_tool(toolkit.get_technical_indicators, ticker, start_date_audit, trade_date)

            try:
                audit_result = auditor_chain.invoke({
                    "raw_data": raw_data,
                    "agent_report": final_state.get('market_report', '')
                })
                append_log(task_id, "äº‹å®ä¸€è‡´æ€§å®¡è®¡ï¼š")
                append_log(task_id, str(audit_result.dict()))
            except Exception as ae:
                err_str = str(ae)
                append_log(task_id, f"å®¡è®¡å¤±è´¥: {err_str}")
                # Fallback: some providers don't support structured response_format. Try a plain prompt and parse JSON.
                if "response_format type is unavailable" in err_str or "invalid_request_error" in err_str:
                    try:
                        import json, re
                        fallback_prompt = (
                            "è¯·æ ¹æ®åŸå§‹æ•°æ®å®¡æ ¸å¸‚åœºæŠ¥å‘Šã€‚è¿”å›ä¸€ä¸ªåŒ…å«é”®çš„ JSON å¯¹è±¡ã€‚: is_consistent (bool), discrepancies (list), justification (string).\n\n"
                            f"åŸå§‹æ•°æ®:\n{raw_data}\n\næ™ºèƒ½ä½“æŠ¥å‘Š:\n{final_state.get('market_report','')}"
                        )
                        raw = deep_thinking_llm.invoke(fallback_prompt).content
                        m = re.search(r"\{.*\}", raw, re.S)
                        if m:
                            js = json.loads(m.group(0))
                        else:
                            js = json.loads(raw)
                        append_log(task_id, f"å®¡è®¡å›é€€ç»“æœ: \n ä¸€è‡´æ€§: {js['is_consistent']} \n å·®å¼‚ç‚¹: {js['discrepancies']} \n å®¡è®¡è¯´æ˜: {js['justification']}")
                    except Exception as e2:
                        append_log(task_id, f"å®¡è®¡å›é€€å¤±è´¥: {e2}")
        except Exception as e:
            append_log(task_id, f"å®¡è®¡å¤±è´¥: {str(e)}")

        # 7. ä»»åŠ¡å®Œæˆ
        try:
            update_progress(task_id, 1.0, "å®Œæˆ")
        except Exception:
            pass
        complete_task(task_id, final_state, final_signal)

    except Exception as e:
        error_msg = f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}\n{traceback.format_exc()}"
        append_log(task_id, error_msg)
        if task_id in task_storage:
            task_storage[task_id]["status"] = "error"
            task_storage[task_id]["error"] = error_msg
